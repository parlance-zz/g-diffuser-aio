# ---- Model definitions ----

# You can either specify models inside the engine definition, or specify them first.
# Putting them first like this allows them to be used multiple times while only loading 
# them off disk once, reducing RAM usage and speeding up loading engines that share model files

# We start off by defining a single model, in this case the Stability VAE.
# Model definitions must have a unqiue model_id which is how they are referenced later.
- model_id: "stability-vae-ema"
  # This is only a vae model, not a pipeline, so we must tell the server that
  type: "vae"
  # There isn't an fp16 version of the model available, so always load the fp32 version.
  # If you have fp16 models available, it reduces RAM usage to use them, but the server
  # will convert the fp32 model if it needs an fp16 model and one doesn't exist.
  has_fp16: False
  # This is the HuggingFace model ID to load from if the local version isn't found.
  # You can leave it out, but then the model will fail to load if the local files don't exist
  model: "stabilityai/sd-vae-ft-ema"
  # This is a path (relative or absolute) to look for a local copy of the fp32 model.
  # You can leave it out if you want to only load from HuggingFace
  local_model: "./sd-vae-ft-ema"

# And define another VAE. None of the engines reference this VAE by default.
# If you define a model but never use it, it doesn't get loaded, so this won't use any RAM.
- model_id: "stability-vae-mse"
  type: "vae"
  has_fp16: False
  model: "stabilityai/sd-vae-ft-mse"
  local_model: "./sd-vae-ft-mse"

# Here we define the inpainting pipeline. Since only the unet is different to the models in
# the V1.5 pipeline, we'll only load that
- model_id: "stable-diffusion-inpainting"
  # We don't need a type field here, as pipeline is the default
  #   type: "pipeline"
  # We only use the unet from this model, so only load that. You could also
  # pass a list of submodules here
  whitelist: "unet"
  # You've seen this model field above, it's the HuggingFace model ID
  model: "runwayml/stable-diffusion-inpainting"
  # If loading from HuggingFace, for this model we need to provide an authorization token
  use_auth_token: True
  # As above, this is the path to look for a local copy of the fp32 model.
  local_model: "./stable-diffusion-inpainting"
  # Because this model has fp16 weights available, you can also specify where a local
  # copy of those can be found. You can leave either of these local_model fields out if
  # you know you won't use that weight.
  local_model_fp16: "./stable-diffusion-inpainting-fp16"

# Here we define the base v1.5 Stable Diffusion pipeline model. 
- model_id: "stable-diffusion-v1-5-base"
  # We blacklist the vae as we will be replacing it in the overrides, and this saves a little RAM
  blacklist: "vae"
  # You've see these next few fields already - this is the HuggingFace model ID
  model: "runwayml/stable-diffusion-v1-5"
  # .. saying we need to authorise to download
  use_auth_token: True
  # .. where the local fp32 model is
  local_model: "./stable-diffusion-v1-5"
  # .. and where the local fp16 model is
  local_model_fp16: "./stable-diffusion-v1-5-fp16"
  # Now we provide two additional models to load into the pipeline.
  overrides:
    # We load the inpaint_unet here (the pipeline otherwise wouldn't have one and would fall back
    # to the previous model-independant inpaintint algorithm.)
    # Because we're using a single model from a whole pipeline, we need to specify to use the unet.
    inpaint_unet: "@stable-diffusion-inpainting/unet"
    # We override the vae with a different (better) one here.
    # Because we're using a simple model, we just reference it directly.
    vae: "@stability-vae-ema"

# Here are two CLIP models. These are used in the CLIP guidance engines.
# We're going to stop commenting on fields you've seen before now
- model_id: "laion-clip-h"
  # Clip models can be loaded as clip_model or feature_extractor. By using a type of clip
  # a pipeline is created with just clip_model and feature_extractor models.
  # You _could_ specify clip_model or feature extractor here to just load that type of model
  type: "clip"
  model: "laion/CLIP-ViT-H-14-laion2B-s32B-b79K"
  has_fp16: False
# None of the engines reference this CLIP model. If you define a model 
# but never use it, it doesn't get loaded, so this won't use any RAM
- model_id: "laion-clip-l"
  type: "clip"
  model: "laion/CLIP-ViT-L-14-laion2B-s32B-b82K"
  has_fp16: False
- model_id: "laion-clip-b"
  type: "clip"
  model: "laion/CLIP-ViT-B-32-laion2B-s34B-b79K"
  has_fp16: False

# The Waifu Diffusion V1.3 mode. By now everything should make sense.
- model_id: "waifu-diffusion-v1-3-base"
  blacklist: "vae"
  model: "hakurei/waifu-diffusion"
  local_model: "./waifu-diffusion-v1-3"
  local_model_fp16: "./waifu-diffusion-v1-3-fp16"
  use_auth_token: False
  overrides:
    vae: "@stability-vae-ema"

# Now we start specifying the Stable Diffusion 2 models. These follow 
# the same structure as the 1.5 models above
- model_id: "stable-diffusion-v2-inpainting"
  model: "stabilityai/stable-diffusion-2-inpainting"
  whitelist: ["text_encoder", "unet"]
  local_model: "./stable-diffusion-v2-inpainting"
  local_model_fp16: "./stable-diffusion-v2-inpainting-fp16"

- model_id: "stable-diffusion-v2-base"
  model: "stabilityai/stable-diffusion-2-base"
  local_model: "./stable-diffusion-v2-base"
  local_model_fp16: "./stable-diffusion-v2-base"
  overrides:
    inpaint_unet: "@stable-diffusion-v2-inpainting/unet"

# And the 2.1 base (using the 2.0 inpaint until 2.1 inpaint model comes out)
- model_id: "stable-diffusion-v2-1-base"
  model: "stabilityai/stable-diffusion-2-1-base"
  local_model: "./stable-diffusion-v2-1-base"
  local_model_fp16: "./stable-diffusion-v2-1-base"
  overrides:
    inpaint_unet: "@stable-diffusion-v2-inpainting/unet"

# At the moment, the higher resolution 768x768 Stable Diffusion 2 model
# is set up as a seperate engine, as the V2 inpaint model is still 512x512
# and sdgrpcserver can't handle mixed resolutions like that.
- model_id: "stable-diffusion-v2-vpred"
  model: "stabilityai/stable-diffusion-2"
  local_model: "./stable-diffusion-v2"
  local_model_fp16: "./stable-diffusion-v2-fp16"

# And 2.1 768x768
- model_id: "stable-diffusion-v2-1-vpred"
  model: "stabilityai/stable-diffusion-2-1"
  local_model: "./stable-diffusion-v2-1"
  local_model_fp16: "./stable-diffusion-v2-1-fp16"

# This is the Inkpunk community finetune of Stable Diffusion 2. This shows
# how to set up a grafted inpaint pipeline on a V2 base
- model_id: 'inkpunk-diffusion'
  model: "Envvi/Inkpunk-Diffusion"
  has_fp16: False
  overrides:
    inpaint_unet:
      model: "@stable-diffusion-v2-inpainting/unet"
    # The Inkpunk main text_encoder is incompatible with the SD2 inpaint text encoder
    # so we need to override that too
    inpaint_text_encoder:
      model: "@stable-diffusion-v2-inpainting/text_encoder"

# ---- Engine definitions ----

# Now we define a set of engines that will handle requests. These expect
# to be passed a classname to use to actually build the engine with, a model
# (either from the definitions above or just declared inline) and maybe some options

# This is the Stable Diffusion V1.5 model with CLIP guidance enabled. It's the standard engine
# we currently recommend to give the best results on most people's hardware. This configuration 
# needs much less VRAM but is slightly less accurate than -quality below
- id: "stable-diffusion-v1-5-standard"
  # You can set enabled to False to temporarily prevent it from being loaded
  enabled: True
  # You can set visible to False to make the engine available but prevent it from showing 
  # up in the API list (directly requesting it works)
  visible: True
  # This name can be used by User Interfaces
  name: "Stable Diffusion V1.5 w/ standard CLIP guidance"
  # This description can be used by User Interfaces
  description: "Stable Diffusion using the RunwayML model, CLIP guidance (standard config) and our Unified pipeline"
  # This class is used to actually handle the generation.
  class: "UnifiedPipeline"
  # You can set various options on each engine.
  options:
    # We use less cutouts than quality CLIP guidance, avoiding using the VAE entirely
    clip:
      approx_cutouts: 2
      vae_cutouts: 0
  # Here we set the model to use - in this case we're just referencing an already-loaded model
  model: "@stable-diffusion-v1-5-base"
  # You can provide individual model overrides in engines just like in models
  overrides:
    # Just like in models, clip means both clip_model and feature_extractor
    clip: "@laion-clip-b"

# This is the Stable Diffusion V1.5 model with CLIP guidance enabled. This configuration needs quite
# a lot of VRAM, but is faster and produces better results if you have enough.
- id: "stable-diffusion-v1-5-quality"
  # If you'd like to test this engine is compatible with your hardware, set this to True. 
  enabled: False
  visible: True
  name: "Stable Diffusion V1.5 w/ quality CLIP guided"
  description: "Stable Diffusion using the RunwayML model, CLIP guidance (quality config) and our Unified pipeline"
  class: "UnifiedPipeline"
  # Here we're setting some options for the CLIP guidance engine
  options:
    clip:
      approx_cutouts: 2
      vae_cutouts: 2
  model: "@stable-diffusion-v1-5-base"
  # You can provide individual model overrides in engines just like in models
  overrides:
    # Just like in models, clip means both clip_model and feature_extractor
    clip: "@laion-clip-h"

# This is the main Stable Diffusion V1.5 model
- id: "stable-diffusion-v1-5-noclip"
  # This engine is disabled by default, since it's a strict subset of the engines above.
  # It uses slightly less VRAM because no clip model is loaded. You can enable it by changed this to True
  enabled: False
  # You can set visible to False to make the engine available but prevent it from showing 
  # up in the API list (directly requesting it works)
  visible: True
  # This name can be used by User Interfaces
  name: "Stable Diffusion V1.5 without CLIP guidance"
  # This description can be used by User Interfaces
  description: "Stable Diffusion using the RunwayML model and our Unified pipeline"
  # This class is used to actually handle the generation.
  class: "UnifiedPipeline"
  # And finally we set the model to use - in this case we're just referencing an already-loaded model
  model: "@stable-diffusion-v1-5-base"

# This is the Stable Diffusion V2.1 model with CLIP guidance enabled.
- id: "stable-diffusion-v2-1-standard"
  enabled: True
  visible: True
  name: "Stable Diffusion V2.1 w/ standard CLIP guidance"
  description: "Stable Diffusion using the Stability V2.1 512x512 model, CLIP guidance (standard config) and our Unified pipeline"
  class: "UnifiedPipeline"
  options:
    clip:
      approx_cutouts: 2
      vae_cutouts: 0
  model: "@stable-diffusion-v2-1-base"
  overrides:
    clip: "@laion-clip-b"

# This is the Stable Diffusion V2.1 model with CLIP guidance enabled. This configuration needs quite
# a lot of VRAM, but is faster and produces better results if you have enough.
- id: "stable-diffusion-v2-1-quality"
  # If you'd like to test this engine is compatible with your hardware, set this to True. 
  enabled: False
  visible: True
  name: "Stable Diffusion V2.1 w/ quality CLIP guidance"
  description: "Stable Diffusion using the Stability V2. 512x512 model, CLIP guidance (quality config) and our Unified pipeline"
  class: "UnifiedPipeline"
  options:
    clip:
      approx_cutouts: 2
      vae_cutouts: 2
  model: "@stable-diffusion-v2-1-base"
  overrides:
    clip: "@laion-clip-l"

# This is the Stable Diffusion V2.1 768x768 model with CLIP guidance. This configuration needs quite
# a lot of VRAM, but is faster and produces better results if you have enough.
- id: "stable-diffusion-v2-1-vpred"
  # If you'd like to test this engine is compatible with your hardware, set this to True. 
  enabled: False
  visible: True
  name: "Stable Diffusion V2.1 VPred w/ quality CLIP guidance"
  description: "Stable Diffusion using the Stability V2.1 768x768 model, CLIP guidance (quality config) and our Unified pipeline"
  class: "UnifiedPipeline"
  options:
    clip:
      approx_cutouts: 2
      vae_cutouts: 2
  model: "@stable-diffusion-v2-1-vpred"
  overrides:
    clip: "@laion-clip-l"

- id: "inkpunk-diffusion"
  # To save on initial download time, the Inkpunk Diffusion model is disabled
  # by default. This one should work on any hardware that can run the main model though, 
  # so anyone can enable it if they want to.
  enabled: False
  visible: True
  name: "Inkpunk Diffusion"
  description: "Stable Diffusion using the Inkpunk Diffusion model and our Unified pipeline"
  class: "UnifiedPipeline"
  model: "@inkpunk-diffusion"
  options:
    grafted_inpaint: True
    clip:
      approx_cutouts: 2
      vae_cutouts: 0
  model: "@inkpunk-diffusion"
  overrides:
    clip: "@laion-clip-b"

# This is the standard Waifu Diffusion V1.3 model
- id: "waifu-diffusion-v1-3-standard"
  # To save on initial download time, all the Waifu Diffusion models are disabled
  # by default. This one should work on any hardware that can run the main model though, 
  # so anyone can enable it if they want to.
  enabled: False
  visible: True
  name: "Waifu Diffusion V1.3"
  description: "Stable Diffusion using the Hakurei Waifu Diffusion model and our Unified pipeline"
  class: "UnifiedPipeline"
  model: "@waifu-diffusion-v1-3-base"
  options:
    grafted_inpaint: True
    clip:
      approx_cutouts: 2
      vae_cutouts: 0
  overrides:
    inpaint_unet: "@stable-diffusion-inpainting/unet"
    clip: "@laion-clip-b"

# This is the quality (high VRAM) Waifu Diffusion V1.3 model
- id: "waifu-diffusion-v1-3-quality"
  # To save on initial download time, all the Waifu Diffusion models are disabled
  # by default.
  enabled: False
  visible: True
  name: "Waifu Diffusion V1.3"
  description: "Stable Diffusion using the Hakurei Waifu Diffusion model and our Unified pipeline"
  class: "UnifiedPipeline"
  model: "@waifu-diffusion-v1-3-base"
  options:
    grafted_inpaint: True
    clip:
      approx_cutouts: 2
      vae_cutouts: 2
  overrides:
    inpaint_unet: "@stable-diffusion-inpainting/unet"
    clip: "@laion-clip-h"

# And finally, here is the unmodified Stable Diffusion V1.4 model, showing how to list
# the model inline. This is disabled by default, set enabled: True to enable it.
- id: "stable-diffusion-v1-4"
  enabled: False
  visible: True
  name: "Stable Diffusion V1.4"
  description: "Stable Diffusion using the CompVis model and our Unified pipeline"
  class: "UnifiedPipeline"
  model: "CompVis/stable-diffusion-v1-4"
  local_model: "./stable-diffusion-v1-4"
  local_model_fp16: "./stable-diffusion-v1-4-fp16"
  use_auth_token: True
